{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.special import softmax\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for similarity\n",
    "similarity_threshold = 0.3\n",
    "#index for narrration qualitative visualization\n",
    "narr_index = 0\n",
    "# Duration of each video feature\n",
    "feature_duration = 2.1333\n",
    "#Whether to do grounding or qualitatively display similar segments\n",
    "qual_vis = False\n",
    "#number of high similarity segments for comparison (qual_vis only)\n",
    "top_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to seconds for plotting\n",
    "def timestamp_to_seconds(timestamp):\n",
    "    return timestamp.dt.hour * 3600 + timestamp.dt.minute * 60 + timestamp.dt.second + timestamp.dt.microsecond / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frame_based_grounding(narrations_df, similarity_scores, feature_duration):\n",
    "    narrations = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    for idx, row in narrations_df.iterrows():\n",
    "        start = row['narration_seconds']\n",
    "        end = row['narration_seconds']\n",
    "        \n",
    "        while True:\n",
    "            expand_left = expand_right = False\n",
    "            left_index = int(start / feature_duration) - 1\n",
    "            right_index = int(end / feature_duration) + 1\n",
    "            \n",
    "            if left_index >= 0 and similarity_scores[idx][left_index] >= similarity_threshold:\n",
    "                start -= feature_duration\n",
    "                expand_left = True\n",
    "            \n",
    "            if right_index < similarity_scores.shape[1] and similarity_scores[idx][right_index] >= similarity_threshold:\n",
    "                end += feature_duration\n",
    "                expand_right = True\n",
    "            \n",
    "            # Break the loop if neither side can be expanded\n",
    "            if not expand_left and not expand_right:\n",
    "                break\n",
    "        narrations.append(row['narration_id'])\n",
    "        start_times.append(start)\n",
    "        end_times.append(end)\n",
    "        \"\"\" print(f\"Expanded Ground Truth Interval: {start:.2f} to {end:.2f} seconds\")\n",
    "        print(f\"Ground Truth Narration: {row['narration']} at {row['narration_timestamp']}\")\n",
    "        print() \"\"\"\n",
    "    return narrations, start_times, end_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity_scores(narration_features, video_features, use_qual_vis=False):\n",
    "    if qual_vis:\n",
    "        similarity_scores = cosine_similarity(narration_features, video_features, dim=1)\n",
    "    else:\n",
    "        dot_product = torch.matmul(narration_features, video_features.transpose(-1, -2))\n",
    "        narration_norm = torch.linalg.norm(narration_features, dim=-1, keepdim=True)\n",
    "        video_norm = torch.linalg.norm(video_features, dim=-1, keepdim=True)\n",
    "        similarity_scores = torch.div(dot_product, torch.mul(narration_norm, video_norm.T))\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qual_vis(narrations_df, similarity_scores):\n",
    "    top_indices = torch.topk(similarity_scores, top_n).indices.flatten()\n",
    "    start_times = top_indices * feature_duration\n",
    "    end_times = start_times + feature_duration\n",
    "    print(\"Current narration:\", narrations_df.iloc[narr_index]['narration'])\n",
    "    print(f\"Ground Truth Interval: {narrations_df.iloc[narr_index]['start_seconds']:.2f} to {narrations_df.iloc[narr_index]['stop_seconds']:.2f} seconds\")\n",
    "    for start, end in zip(start_times.numpy().flatten(), end_times.numpy().flatten()):\n",
    "        overlapping_narrations = narrations_df[(narrations_df['start_seconds'] <= end) & (narrations_df['stop_seconds'] >= start)]\n",
    "        print(f\"High Similarity Interval: {start:.2f} to {end:.2f} seconds\")\n",
    "        if not overlapping_narrations.empty:\n",
    "            print(\"Overlapping Ground Truth Narrations:\")\n",
    "            for _, row in overlapping_narrations.iterrows():\n",
    "                print(f\"  - {row['narration']} from {row['start_timestamp']} to {row['stop_timestamp']}\")\n",
    "        else:\n",
    "            print(\"  - No overlapping narrations.\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2400377/783235174.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  narrations_df['start_seconds'] = timestamp_to_seconds(pd.to_datetime(narrations_df['start_timestamp']))\n",
      "/tmp/ipykernel_2400377/783235174.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  narrations_df['stop_seconds'] = timestamp_to_seconds(pd.to_datetime(narrations_df['stop_timestamp']))\n",
      "/tmp/ipykernel_2400377/783235174.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  narrations_df['narration_timestamp'] = pd.to_datetime(narrations_df['narration_timestamp'])\n"
     ]
    }
   ],
   "source": [
    "#Process narrations\n",
    "narrations_df = pd.read_csv('/private/home/arjunrs1/epic-kitchens-100-annotations/EPIC_100_train.csv')\n",
    "narrations_df = narrations_df[narrations_df.video_id==\"P01_01\"] #TODO: change from hardcoded\n",
    "narrations_df['start_seconds'] = timestamp_to_seconds(pd.to_datetime(narrations_df['start_timestamp']))\n",
    "narrations_df['stop_seconds'] = timestamp_to_seconds(pd.to_datetime(narrations_df['stop_timestamp']))\n",
    "narrations_df['narration_timestamp'] = pd.to_datetime(narrations_df['narration_timestamp'])\n",
    "narrations_df['narration_seconds'] = timestamp_to_seconds(narrations_df['narration_timestamp'])\n",
    "narrations_df = narrations_df[narrations_df.stop_seconds<=1620] #TODO: Change from hardcoded\n",
    "narrations_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Load video features\n",
    "video_features = torch.load(\"/private/home/arjunrs1/EgoVLPv2/EgoVLPv2/video_features_ng/00:00:01.089.pt\") #TODO: change from hardcoded\n",
    "\n",
    "#load narration_features\n",
    "if qual_vis:\n",
    "    first_narration_id = narrations_df.iloc[0]['narration_id']\n",
    "    narration_features = torch.load(f'/private/home/arjunrs1/EgoVLPv2/EgoVLPv2/narration_features_ng/{first_narration_id}.pt').cpu()\n",
    "else:\n",
    "    narration_features = []\n",
    "    for _, row in narrations_df.iterrows():\n",
    "        narration_features.append(torch.load(f'/private/home/arjunrs1/EgoVLPv2/EgoVLPv2/narration_features_ng/{row.narration_id}.pt').cpu())\n",
    "    narration_features = torch.concat(narration_features)\n",
    "narration_features = narration_features.unsqueeze(0) if narration_features.dim() == 1 else narration_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = generate_similarity_scores(narration_features, video_features, use_qual_vis=qual_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qual_vis:\n",
    "    generate_qual_vis(narrations_df, similarity_scores)\n",
    "else:\n",
    "    narrations, start_times, end_times = generate_frame_based_grounding(narrations_df, similarity_scores, feature_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "narration_grounded = pd.DataFrame({\n",
    "    'narration_id': narrations,\n",
    "    'start_seconds': start_times,\n",
    "    'stop_seconds': end_times\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post-processing to format as the audio_grounded narrations df is:\n",
    "narration_grounded['assigned_intervals'] = narration_grounded.apply(lambda row: [[row['start_seconds'], row['stop_seconds']]], axis=1)\n",
    "narration_grounded = narration_grounded.drop(['start_seconds', 'stop_seconds'], axis=1)\n",
    "merged_df = pd.merge(narrations_df, narration_grounded, on='narration_id')\n",
    "merged_df = merged_df[['start_timestamp', 'stop_timestamp', 'narration', 'start_seconds', 'stop_seconds', 'assigned_intervals']]\n",
    "merged_df = merged_df.sort_values(by='start_timestamp')\n",
    "merged_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_grounded_narrs_output_dir = \"video_grounded_narrations\"\n",
    "video_grounded_narrs_filename = f\"similarity_threshold={similarity_threshold}_feature_duration={feature_duration}.pkl\"\n",
    "video_grounded_narrations_filepath = os.path.join(\"/private/home/arjunrs1/epic-sounds-annotations\", video_grounded_narrs_output_dir, video_grounded_narrs_filename)\n",
    "merged_df.to_pickle(video_grounded_narrations_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have computed EgoVLP video features at ~2 second resolution from the video (non-overlapping), and computed EgoVLP narration features. Then we verified qualitatively that regions with high narr-video\n",
    "#similarity corresponded to regions where the narration was active. Therefore, we have verified that the narration/video embeddings are proper. Below are the steps to further improve this:\n",
    "\n",
    "#TODO: Modify video feature generation:\n",
    "    #1) Create separate .csv file with short (1 second) overlapping windows (0.5 second) on each row. No other cols\n",
    "    # are needed in this .csv file. \n",
    "    #2) Load in this csv file and load in each 1 second window of video, using num_segments=16 to subsample the video\n",
    "    #into 16 frames. NOTE: Ensure that you check that this is 16 frames in the generate_*.py script.\n",
    "    #3) Use EK-100 finetuned checkpoint instead of pre-trained checkpoint.\n",
    "#TODO: Show that with more fine-grained features, we immprove grounding (going from coarse 2 second features to more-finegrained 1 or 1.5 second features)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
